# -*- coding: utf-8 -*-
"""Chatbot4Derma - Final Project - NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yj44IclepC0wCtQCj2jL0K98lIop6pZ-

**✅ Install Required Packages**


*   **chromadb**: For storing and retrieving data using vector search techniques.
*   **sentence-transformers**: For creating numerical representations (embeddings) of sentences.
* **pandas**: For analyzing data and loading CSV files.
* **matplotlib**: For graphing.
* **scikit-learn**: For statistical evaluation (precision, recall, etc.).
"""

!pip install -q chromadb sentence-transformers pandas scikit-learn

"""**✅ Import Libraries**
* Import all the tools used to build, retrieve "RAG", and analyze data.
"""

import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from sentence_transformers import SentenceTransformer
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import random

"""Upload medical dataset related to dermatology from **CSV**
* The file containing the dermatology data is uploaded.
* dropna: Deletes any row with a missing title or content.
"""

file_path = "/content/Dataset_Chatbot4Derma_Scraped_Articles.csv"

# Load CSV and drop nulls
#raw_df = pd.read_csv(file_path).dropna(subset=["Title", "Content"])
raw_df = pd.read_csv(file_path, encoding='windows-1256').dropna(subset=["Title", "Content"])

"""**Info about dataset count of row and column**"""

raw_df.count()

"""**Display 5 sample from dataset**"""

raw_df.sample(n=10)

"""**✅ Initialize & Create a ChromaDB database**
* ChromaDB is a radial database created to store and retrieve documents based on semantic similarity using the HNSW algorithm and Cossian distance.
"""

chroma_client = chromadb.Client()
collection = chroma_client.create_collection(
    name="derma_knowledge_base",
    metadata={"hnsw:space": "cosine"}
)

"""**✅ Create a representative model (Embedding)**
* The MiniLM model is used to generate vector representations of sentences.
* This model is multilingual
"""

embedder = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")  # multilingual & Colab-safe

"""**✅ Storing data within ChromaDB**
* Here each document (skin disease) is entered into the database with its information (title - author - medical classification).
"""

for idx, row in raw_df.iterrows():
    doc_id = f"doc_{idx}"
    content = row["Content"]
    metadata = {
        "title": row["Title"],
        "author": row["Author"],
        "category": row["Categories"]
    }
    print('Row No. '+str(idx))
    collection.add(documents=[content], ids=[doc_id], metadatas=[metadata])

print("✅ ChromaDB successfully populated with source-aware dermatology knowledge base")

"""**✅ Execute a RAG (Retrieval-Augmented Generation) query**
* It takes a query from the user and converts it to a numerical representation.
* Top_k retrieves the documents from the database that are most meaningful to the query.
* It displays the most important sources and their content.
"""

def rag_query(query, top_k=3, canPrint=True):
    # Embed query
    query_vector = embedder(query)
    # Retrieve
    results = collection.query(
        query_texts=[query],
        n_results=top_k
    )
    # Display answer and sources
    if canPrint:
      for i in range(len(results["documents"][0])):
          print(f"\n🔎 Source {i+1}: {results['metadatas'][0][i]['title']}")
          print(f"📚 Category: {results['metadatas'][0][i]['category']}")
          print(f"📝 Content: {results['documents'][0][i][:1000]}...")
    return results

"""**✅ Evaluation of the accuracy of results**

**Evaluation mechanism:**

* **test_queries**: Test questions to evaluate system performance.
* **expected_keywords**: The keywords that should appear in the correct answer.
* **preds**: The actual results (did the answer contain the keyword?).
* **trues**: A list of true values (used to calculate metrics).

**Then calculate the accuracy:**

* **accuracy**: The percentage of answers that contained the expected words.
* **precision**: Of all the answers the system returned, how many were correct.
* **recall**: Of all the answers it should have known, how many were recognized.
* **f1_score**: A balancing measure between precision and recall.
"""

def evaluate_responses(test_queries, expected_keywords):
    preds, trues = [], []
    for query, expected in zip(test_queries, expected_keywords):
        result = rag_query(query, top_k=1, canPrint=True)
        answer = result["documents"][0][0].lower()
        preds.append(any(kw.lower() in answer for kw in expected))
        trues.append(True)
    #**Now calculate the accuracy:**
    acc = accuracy_score(trues, preds)
    prec = precision_score(trues, preds)
    rec = recall_score(trues, preds)
    f1 = f1_score(trues, preds)
    return acc, prec, rec, f1

"""**✅ Performance testing with real models**
* These are examples of bilingual questions.
* The answer is checked to see if the important words are present.
"""

# Sample Test Cases
sample_queries = [
    "What causes Allergic contact dermatitis?",
    "طرق علاج القرحة قلاعية عند البالغين",
    "How do I identify fungal skin infection?"
]

expected_keywords = [
    ["allergic", "contact", "dermatitis"],
    ["قلاعية", "علاج", "مراهم"],
    ["fungal", "infection", "rash"]
]

# Evaluation
acc, prec, rec, f1 = evaluate_responses(sample_queries, expected_keywords)

print("\nEvaluation Results")
print(f"Accuracy: {acc:.2f}")
print(f"Precision: {prec:.2f}")
print(f"Recall: {rec:.2f}")
print(f"F1 Score: {f1:.2f}")

"""**✅ Performance results chart**"""

# Plotting Performance
metrics = [acc, prec, rec, f1]
labels = ["Accuracy", "Precision", "Recall", "F1 Score"]
colors = ["skyblue", "lightgreen", "salmon", "gold"]

plt.figure(figsize=(8, 5))
plt.bar(labels, metrics, color=colors)
plt.ylim(0, 1)
plt.title("Chatbot Performance on Dermatological Queries")
plt.ylabel("Score")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()
plt.savefig("/content/performance_plot.png")
print("Performance plot saved as file in this path /content/performance_plot.png")

from IPython.display import Image
Image("/content/performance_plot.png")

"""**🤖 Connecting a Telegram 📲 bot with a RAG model to analyze user inquiries about skin diseases 🤖**

**✅ Install required packages**
* python-telegram-bot: A popular library for building Telegram bots using Python in an asynchronous manner.
* nest_asyncio: Used to circumvent the limitations of asynchronous loops in environments like Jupyter/Colab
"""

!pip install -q python-telegram-bot nest_asyncio

"""**Import libraries and setting up synchronization loops in Colab**"""

import nest_asyncio
import asyncio
nest_asyncio.apply() #This line allows you to use async/await within the Colab environment (which runs on its own event loop). Without it, errors will appear when running asyncio-based applications like python-telegram-bot.

from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, filters, CommandHandler
# Update: Represents a user message or interaction event.
# ContextTypes: Provides context information for each request (such as bot or session information).
# ApplicationBuilder: A way to build a modern bot application.
# MessageHandler: Receives text messages.
# Filters: Filters the message type (such as text only).
# CommandHandler: Handles commands like /start.

"""**✅ BotFather**
* The token code from BotFather on Telegram.
* This code is what links the bot to your Telegram account.
"""

TELEGRAM_TOKEN = "7623812546:AAH8cU9lRjvSL8WVYFV-46AXN0zcVb-_ys8"

"""**✅ Dealing with text messages**
* When a message is received from the user, the text is extracted and the query is parsed via the rag_query() function.
* Then Execute a RAG query and respond
* rag_query: Executes a RAG query and retrieves text segments from ChromaDB.
* The content is summarized, combining the title, label, and content into a single message.
* parse_mode="Markdown" enables the use of formatting such as bold headings.
"""

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_query = update.message.text
    await update.message.reply_text("🔍  جاري تحليل استفسارك يرجى الانتظار ...")

    try:
        results = rag_query(user_query, top_k=1, canPrint=False)
        response = ""

        for i in range(len(results["documents"][0])):
            title = results['metadatas'][0][i]['title']
            cat = results['metadatas'][0][i]['category']
            content = results['documents'][0][i][:1000] + "..."
            response += f"\n🩺 العنوان: *{title}*\n📚 التصنيف: {cat}\n\n📝 الإجابة: {content}\n\n"

        await update.message.reply_text(response, parse_mode="Markdown")

    except Exception as e:
        await update.message.reply_text(f"Error : {str(e)}")

"""**This is the welcome response when sending /start.**"""

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("مرحباً بك في روبوت الأمراض الجلدية! أرسل وصفاً أو سؤالاً عن الأعراض لتلقي تحليل طبي مبدئي.")

"""**Bind processors and run the bot**"""

# Bot settings
app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
app.add_handler(CommandHandler("start", start))
app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

# Run bot inner colab
await app.initialize()
await app.start()
print("🤖 Telegram bot is running. You can now send messages.")
await app.updater.start_polling()